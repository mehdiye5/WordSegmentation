{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>ensegment: default program</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from default import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "In the following cell we are showing a smoothing function called avoid_long_words. This smoothing function is an improved version of the smoothing function implemented in \"Beautiful Data\" by P. Norvig. In our implementation of the smoothing function we are increasing the magnitude of the penalty for long words.\n",
    "\n",
    "We are then initializing the wordcout and segmenter objects, before iterating through the dev.txt text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "choose spain\n",
      "this is a test\n",
      "who represents\n",
      "experts exchange\n",
      "speed of art\n",
      "un climate change body\n",
      "we are the people\n",
      "mention your faves\n",
      "now playing\n",
      "the walking dead\n",
      "follow me\n",
      "we are the people\n",
      "mention your faves\n",
      "check domain\n",
      "big rock\n",
      "name cheap\n",
      "apple domains\n",
      "honesty hour\n",
      "being human\n",
      "follow back\n",
      "social media\n",
      "30 seconds to earth\n",
      "current rate sought to go down\n",
      "this is insane\n",
      "what is my name\n",
      "is it time\n",
      "let us go\n",
      "me too\n",
      "now thatcher is dead\n",
      "advice for young journalists\n"
     ]
    }
   ],
   "source": [
    "# Smoothing function for missing words\n",
    "def avoid_long_words(word, N):\n",
    "    \"Estimate the probability of an unknown word.\"\n",
    "    return 10./(N*30**len(word))\n",
    "\n",
    "# Initializing wordcounter and segmenter\n",
    "Pw = Pdist(data=datafile(\"data/count_1w.txt\"), missingfn=avoid_long_words)\n",
    "segmenter = Segment(Pw)\n",
    "\n",
    "# Iterating .txt file rows\n",
    "with open(\"data/input/dev.txt\") as f:\n",
    "    for line in f:\n",
    "        print(\" \".join(segmenter.segment(line.strip())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "The final iteration of default.py resulted in an F1 score of 1.00 on the dev.txt file. The only change that was needed to achieve this score was to create a custom smoothing function for unseen words. This function provided an estimate of the probability for each unseen word by assigning a lower probability to longer words. This solution also performed better on longer, more complex sequences than other techniques we tried.\n",
    "\n",
    "We came across this solution after noticing the code comments in the orginal default.py file. We should have read more carefully at the start!\n",
    "\n",
    "```\n",
    "def avoid_long_words(word, N):\n",
    "    \"Estimate the probability of an unknown word.\"\n",
    "    return 10./(N*30**len(word))\n",
    "```\n",
    "\n",
    "## Other Techniques\n",
    "\n",
    "#### Decreasing the max size of the word\n",
    "\n",
    "This was the first strategy we implemented to increase the performance of our model. By testing different values for the \"L\" parameter in the Segment.splits() function, we were able to reach an F1-score of 0.96 on dev.txt when L was either 11 or 12. This was promising, but we were still unable to correctly classify the \"30secondstoearth\" line.\n",
    "\n",
    "```\n",
    "def splits(self, text, L=12):\n",
    "    \"Return a list of all possible (first, rem) pairs, len(first)<=L.\"\n",
    "    return [(text[:i+1], text[i+1:]) \n",
    "            for i in range(min(len(text), L))]\n",
    "```\n",
    "\n",
    "#### Passing when numbers are in both words\n",
    "\n",
    "The second strategy we implemented was to not include any splits that resulted in numbers in either word. For example, \"30secondstoearth\" could not be split into (\"3\",\"0secondstoearth\"). This function also worked when there were multiple numbers found within a line. Although we were able to achieve an F1 score of dev.txt of 1.00, this was not performing well on more complex sentences.\n",
    "\n",
    "```\n",
    "def splits(self, text, L=12):\n",
    "    \"Return a list of all possible (first, rem) pairs, len(first)<=L.\"\n",
    "    arr = []\n",
    "    for i in range(min(len(text), L)):\n",
    "        # When the split results in numbers in both words, dont split.\n",
    "        if (text[:i+1].isdigit() and any(x.isdigit() for x in text[i+1:])) or (text[i+1:].isdigit() and any(x.isdigit() for x in text[:i+1])):\n",
    "            continue\n",
    "        arr.append((text[:i+1], text[i+1:]))\n",
    "    return arr\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('libexec')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "f081f0fe91c5e21180c1f80c07c56b11659c1efa14f683e264171c96113d81c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
